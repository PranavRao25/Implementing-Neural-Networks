{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c287573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "65bcf2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(ABC):\n",
    "    def __init__(self, label) -> None:\n",
    "        self.label = label\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __call__(self, data1, data2):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _backward(self, grad_out):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class OperationFactory:\n",
    "    def __call__(self, op, node1, node2 = None):\n",
    "        # self.op = op\n",
    "        \n",
    "        if node2 is not None:\n",
    "            if not isinstance(node2, Value):\n",
    "                node2 = Value(node2)\n",
    "            \n",
    "            def _backward():\n",
    "                grad1_update, grad2_update = op._backward(out.grad)\n",
    "                node1.grad += grad1_update\n",
    "                node2.grad += grad2_update\n",
    "            \n",
    "            out = Value(op(node1.data, node2.data), _childern = (node1, node2), _op = op.label)\n",
    "            out._backward = _backward\n",
    "        else:\n",
    "            def _backward():\n",
    "                grad1_update, _ = op._backward(out.grad)\n",
    "                node1.grad += grad1_update\n",
    "            \n",
    "            out = Value(op(node1.data, None), _childern = (node1, ), _op = op.label)\n",
    "            out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "class Addition(Operation):\n",
    "    def __init__(self, label = \"add\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        return data1 + data2\n",
    "    \n",
    "    def _backward(self, grad_out):\n",
    "        return (grad_out, grad_out)\n",
    "\n",
    "class Multiplication(Operation):\n",
    "    def __init__(self, label = \"mul\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        self.data1, self.data2 = data1, data2\n",
    "        return data1 * data2\n",
    "\n",
    "    def _backward(self, grad_out):\n",
    "        return (grad_out * self.data2, grad_out * self.data1)\n",
    "\n",
    "class Subtraction(Operation):\n",
    "    def __init__(self, label = \"sub\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        return data1 - data2\n",
    "\n",
    "    def _backward(self, grad_out):\n",
    "        return (grad_out, grad_out)\n",
    "\n",
    "class Division(Operation):\n",
    "    def __init__(self, label = \"div\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        if data2 == 0:\n",
    "            raise ZeroDivisionError\n",
    "\n",
    "        self.data1, self.data2 = data1, data2\n",
    "        return data1 / data2\n",
    "    \n",
    "    def _backward(self, grad_out):\n",
    "        return (grad_out / self.data2, - self.data1 * grad_out / (self.data2 ** 2))\n",
    "\n",
    "class Power(Operation):\n",
    "    def __init__(self, label = \"pow\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        if not isinstance(data2, (int, float)):\n",
    "            raise TypeError\n",
    "        \n",
    "        self.data1, self.data2 = data1, data2\n",
    "        return data1 ** data2\n",
    "    \n",
    "    def _backward(self, grad_out):\n",
    "        return (grad_out * self.data2 * self.data1 ** (self.data2 - 1), 0.0)\n",
    "\n",
    "class Exp(Operation):\n",
    "    def __init__(self, label = \"exp\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        self.out = math.exp(data1)\n",
    "        return math.exp(data1)\n",
    "    \n",
    "    def _backward(self, grad_out):\n",
    "        return (self.out * grad_out, None)\n",
    "\n",
    "class Tanh(Operation):\n",
    "    def __init__(self, label = \"tanh\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        self.out = math.tanh(data1)\n",
    "        return self.out\n",
    "    \n",
    "    def _backward(self, grad_out):\n",
    "        return ((1 - self.out ** 2) * grad_out, None)\n",
    "\n",
    "class ReLU(Operation):\n",
    "    def __init__(self, label = \"relu\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        self.data = data1\n",
    "        return max(0.0, data1)\n",
    "    \n",
    "    def _backward(self, grad_out):\n",
    "        return (grad_out if self.data > 0.0 else 0.0, None)\n",
    "\n",
    "class LossFunction(ABC):\n",
    "    def __init__(self, label) -> None:\n",
    "        self.label = label\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, data1, data2):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MSE(LossFunction):\n",
    "    def __init__(self, label = \"mse\") -> None:\n",
    "        super().__init__(label)\n",
    "\n",
    "    def __call__(self, data1, data2):\n",
    "        return (data1 - data2) ** 2\n",
    "    \n",
    "class MAE(LossFunction):\n",
    "    def __init__(self, label = \"mae\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        return data1 - data2 if data1 > data2 else data2 - data1\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, label=\"\", _childern = (), _op = '') -> None:\n",
    "        \"\"\"\n",
    "            Value object to store numerical values\n",
    "            :param data      - numerical value\n",
    "            :param label     - label for human readability\n",
    "            :param _childern - all of the childern of the current value node\n",
    "            :param _op       - operation leading to the current value\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self._prev = _childern  # used for backprop (childern is previous)\n",
    "        self._op = _op\n",
    "        self.op_fact = OperationFactory()\n",
    "        self.grad = 0.0  # records the partial derivative of output wrt this node\n",
    "        self._backward = lambda : None  # used for backpropagation\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.data}\"\n",
    "\n",
    "    def operate(self, other, op):\n",
    "        out = self.op_fact(op, node1=self, node2=other)\n",
    "        return out\n",
    "\n",
    "    def __add__(self, other):\n",
    "        add = Addition()\n",
    "        # out = self.op_fact(add, self, other)\n",
    "        # return out\n",
    "        return self.operate(other, add)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        mul = Multiplication()\n",
    "        return self.operate(other, mul)\n",
    "\n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other    \n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        sub = Subtraction()\n",
    "        return self.operate(other, sub)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return self - other\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        division = Division()\n",
    "        return self.operate(other, division)\n",
    "\n",
    "    def __pow__(self, other): # self ** other\n",
    "        pow = Power()\n",
    "        return self.operate(other, pow)\n",
    "\n",
    "    def __rpow__(self, other):  # a ^ x\n",
    "        other = Value(other)\n",
    "        return other ** self\n",
    "    \n",
    "    def exp(self):\n",
    "        exp = Exp()\n",
    "        return self.operate(None, exp)\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self.data <= other.data\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.data > other.data\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.data < other.data\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        return self.data >= other.data\n",
    "    \n",
    "    # def __eq__(self, other):\n",
    "    #     return self.data == other.data\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return self.data != other.data\n",
    "\n",
    "    def act(self, func):\n",
    "        out = self.op_fact(func, self, None)\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        tanh = Tanh()\n",
    "        return self.act(tanh)\n",
    "\n",
    "    def relu(self):\n",
    "        relu = ReLU()\n",
    "        return self.act(relu)\n",
    "\n",
    "    def backward(self):\n",
    "        def build_topo(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                topo.append(node)  # WHERE IT MIGHT GO WRONG\n",
    "                for child in node._prev:\n",
    "                    build_topo(child)\n",
    "        \n",
    "        topo = []\n",
    "        visited = set()\n",
    "        self.grad = 1.0\n",
    "        \n",
    "        build_topo(self)\n",
    "        for node in topo:\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8b4d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n_inputs) -> None:\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(n_inputs)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        act = sum((w.data*i for w,i in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_in, n_out) -> None:\n",
    "        self.neurons = [Neuron(n_in) for _ in range(n_out)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [param for n in self.neurons for param in n.parameters()]\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_input, n_outs) -> None:\n",
    "        ins = [n_input] + n_outs\n",
    "        self.layers = [Layer(ins[i], ins[i+1]) for i in range(len(n_outs))]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        temp = x\n",
    "        for layer in self.layers:\n",
    "            temp = layer(temp)\n",
    "        return temp\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [param for layer in self.layers for param in layer.parameters()]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters():\n",
    "            param.grad = 0\n",
    "\n",
    "# TODO: Documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "06d7a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t, p):\n",
    "    return (t - p) ** 2  # mse\n",
    "\n",
    "X = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "y = [1.0, -1.0, -1.0, 1.0]\n",
    "\n",
    "n_input = 3\n",
    "n_outs = [4, 1]\n",
    "mlp = MLP(n_input, n_outs)\n",
    "\n",
    "lr = 0.01\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cac1f4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 7.6271149299329695\n",
      "1 : 7.623004070113655\n",
      "2 : 7.618829522546875\n",
      "3 : 7.61459026903665\n",
      "4 : 7.610285272300583\n",
      "5 : 7.605913474707976\n",
      "6 : 7.60147379688267\n",
      "7 : 7.59696513616047\n",
      "8 : 7.59238636489067\n",
      "9 : 7.587736328570908\n",
      "10 : 7.583013843804305\n",
      "11 : 7.578217696067626\n",
      "12 : 7.573346637279032\n",
      "13 : 7.568399383153826\n",
      "14 : 7.563374610336575\n",
      "15 : 7.558270953297884\n",
      "16 : 7.5530870009842355\n",
      "17 : 7.547821293209292\n",
      "18 : 7.542472316775301\n",
      "19 : 7.537038501313398\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epochs):\n",
    "    pred = [mlp(x)[0] for x in X]\n",
    "    loss = sum(loss_fn(t, p) for t, p in zip(y, pred))\n",
    "\n",
    "    for param in mlp.parameters():\n",
    "        param.grad = 0  # so that the grad of loss calculated is only for this new iteration\n",
    "    loss.backward()\n",
    "\n",
    "    print(f\"{i} : {loss.data}\")\n",
    "\n",
    "    for param in mlp.parameters():\n",
    "        param.data += -lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c0210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
