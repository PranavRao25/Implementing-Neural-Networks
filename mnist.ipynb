{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f38ff1",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b24262",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe1e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc37e8a",
   "metadata": {},
   "source": [
    "## Required Machinery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074cbefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Operation(ABC):\n",
    "    \"\"\"\n",
    "        Abstract Class defining the functionality for a differentiable operation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label) -> None:\n",
    "        self.label = label\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __call__(self, data1, data2):\n",
    "        \"\"\"\n",
    "            :param data1 - operand 1\n",
    "            :param data2 - operand 2\n",
    "\n",
    "            Returns the computation between the operands\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _backward(self, grad_out):\n",
    "        \"\"\"\n",
    "            :param grad_out - gradient of the result\n",
    "\n",
    "            Returns a tuple of corresponding gradient changes for the operands\n",
    "            leading to the result (to be used in backpropagation)\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "class OperationFactory:\n",
    "    \"\"\"\n",
    "        Takes an Operation object and implements its functions to the computational graph nodes\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, op, node1, node2 = None):\n",
    "        \"\"\"\n",
    "            :param op - Operation to be carried out\n",
    "            :param node1 - operand node 1\n",
    "            :param node2 - operand node 2 (can be None for unary operations)\n",
    "        \"\"\"\n",
    "\n",
    "        if node2 is not None:  # Binary Operation\n",
    "            if not isinstance(node2, Value):\n",
    "                node2 = Value(node2)\n",
    "            \n",
    "            def _backward():\n",
    "                \"\"\"\n",
    "                    This function implements the backward propagation of the gradients from one node\n",
    "                    to its parent nodes\n",
    "                \"\"\"\n",
    "\n",
    "                grad1_update, grad2_update = op._backward(out.grad)\n",
    "                node1.grad += grad1_update\n",
    "                node2.grad += grad2_update\n",
    "            \n",
    "            out = Value(op(node1.data, node2.data), _childern = (node1, node2), _op = op.label)\n",
    "            out._backward = _backward\n",
    "        else:  # Unary Operation\n",
    "            def _backward():\n",
    "                grad1_update, _ = op._backward(out.grad)\n",
    "                node1.grad += grad1_update\n",
    "            \n",
    "            out = Value(op(node1.data, None), _childern = (node1, ), _op = op.label)\n",
    "            out._backward = _backward\n",
    "        return out\n",
    "\n",
    "class Addition(Operation):\n",
    "    def __init__(self, label = \"add\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        return data1 + data2\n",
    "    \n",
    "    def _backward(self, grad_out):\n",
    "        return (grad_out, grad_out)\n",
    "\n",
    "class Multiplication(Operation):\n",
    "    def __init__(self, label = \"mul\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        self.data1, self.data2 = data1, data2\n",
    "        return data1 * data2\n",
    "\n",
    "    def _backward(self, grad_out):\n",
    "        return (grad_out * self.data2, grad_out * self.data1)\n",
    "\n",
    "class Subtraction(Operation):\n",
    "    def __init__(self, label = \"sub\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        return data1 - data2\n",
    "\n",
    "    def _backward(self, grad_out):\n",
    "        return (grad_out, grad_out)\n",
    "\n",
    "class Division(Operation):\n",
    "    def __init__(self, label = \"div\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        if data2 == 0:\n",
    "            raise ZeroDivisionError\n",
    "\n",
    "        self.data1, self.data2 = data1, data2\n",
    "        return data1 / data2\n",
    "    \n",
    "    def _backward(self, grad_out):\n",
    "        return (grad_out / self.data2, - self.data1 * grad_out / (self.data2 ** 2))\n",
    "\n",
    "class Power(Operation):\n",
    "    def __init__(self, label = \"pow\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        if not isinstance(data2, (int, float)):\n",
    "            raise TypeError\n",
    "        \n",
    "        self.data1, self.data2 = data1, data2\n",
    "        return data1 ** data2\n",
    "    \n",
    "    def _backward(self, grad_out):\n",
    "        return (grad_out * self.data2 * self.data1 ** (self.data2 - 1), 0.0)\n",
    "\n",
    "class Exp(Operation):\n",
    "    def __init__(self, label = \"exp\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, _):\n",
    "        self.out = math.exp(data1)\n",
    "        return math.exp(data1)\n",
    "    \n",
    "    def _backward(self, grad_out):\n",
    "        return (self.out * grad_out, None)  # None as unary operation\n",
    "\n",
    "class Tanh(Operation):\n",
    "    def __init__(self, label = \"tanh\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        self.out = math.tanh(data1)\n",
    "        return self.out\n",
    "    \n",
    "    def _backward(self, grad_out):\n",
    "        return ((1 - self.out ** 2) * grad_out, None)\n",
    "\n",
    "class ReLU(Operation):\n",
    "    def __init__(self, label = \"relu\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        self.data = data1\n",
    "        return max(0.0, data1)\n",
    "    \n",
    "    def _backward(self, grad_out):\n",
    "        return (grad_out if self.data > 0.0 else 0.0, None)\n",
    "\n",
    "class LossFunction(ABC):\n",
    "    \"\"\"\n",
    "        Define the functionality of a loss function\n",
    "        No backward call as this function is considered as a composite function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label) -> None:\n",
    "        self.label = label\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, data1, data2):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MSE(LossFunction):\n",
    "    \"\"\"\n",
    "        Euclidean or L2 Norm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label = \"mse\") -> None:\n",
    "        super().__init__(label)\n",
    "\n",
    "    def __call__(self, data1, data2):\n",
    "        return (data1 - data2) ** 2\n",
    "    \n",
    "class MAE(LossFunction):\n",
    "    \"\"\"\n",
    "        Manhattan or L1 Norm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label = \"mae\") -> None:\n",
    "        super().__init__(label)\n",
    "    \n",
    "    def __call__(self, data1, data2):\n",
    "        return data1 - data2 if data1 > data2 else data2 - data1\n",
    "\n",
    "class Value:\n",
    "    \"\"\"\n",
    "        Basic node in the computational graph\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, label=\"\", _childern = (), _op = '') -> None:\n",
    "        \"\"\"\n",
    "            Value object to store numerical values\n",
    "            :param data      - numerical value\n",
    "            :param label     - label for human readability\n",
    "            :param _childern - all of the childern of the current value node\n",
    "            :param _op       - operation leading to the current value\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self._prev = set(_childern)  # used for backprop (childern is previous)\n",
    "        self._op = _op\n",
    "        self.op_fact = OperationFactory()  # to create operations\n",
    "        self.grad = 0.0  # records the partial derivative of output wrt this node\n",
    "        self._backward = lambda : None  # used for backpropagation\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.data}\"\n",
    "\n",
    "    def operate(self, other, op):\n",
    "        \"\"\"\n",
    "            :param other - other Value object\n",
    "            :param op    - defined Operation object\n",
    "\n",
    "            Creates a node with custom operation\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.op_fact(op, node1=self, node2=other)\n",
    "        return out\n",
    "\n",
    "    def __add__(self, other):\n",
    "        add = Addition()\n",
    "        return self.operate(other, add)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        mul = Multiplication()\n",
    "        return self.operate(other, mul)\n",
    "\n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other    \n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        sub = Subtraction()\n",
    "        return self.operate(other, sub)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return self - other\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        division = Division()\n",
    "        return self.operate(other, division)\n",
    "\n",
    "    def __pow__(self, other): # self ** other\n",
    "        pow = Power()\n",
    "        return self.operate(other, pow)\n",
    "\n",
    "    def __rpow__(self, other):  # a ^ x\n",
    "        other = Value(other)\n",
    "        return other ** self\n",
    "    \n",
    "    def exp(self):\n",
    "        exp = Exp()\n",
    "        return self.operate(None, exp)\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self.data <= other.data\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.data > other.data\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.data < other.data\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        return self.data >= other.data\n",
    "\n",
    "    def act(self, func):\n",
    "        \"\"\"\n",
    "            Activation function\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.op_fact(func, self, None)\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        tanh = Tanh()\n",
    "        return self.act(tanh)\n",
    "\n",
    "    def relu(self):\n",
    "        relu = ReLU()\n",
    "        return self.act(relu)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "            Performs back propagation on the current node\n",
    "            To be used only on the final node (sink) of the graph\n",
    "        \"\"\"\n",
    "\n",
    "        def build_topo(node):\n",
    "            \"\"\"\n",
    "                Build a reverse topological sort of the graph\n",
    "            \"\"\"\n",
    "\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                topo.append(node)  # WHERE IT MIGHT GO WRONG\n",
    "                for child in node._prev:\n",
    "                    build_topo(child)\n",
    "        \n",
    "        topo = []\n",
    "        visited = set()\n",
    "        self.grad = 1.0\n",
    "        \n",
    "        build_topo(self)\n",
    "        for node in topo:\n",
    "            node._backward()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e3a025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(ABC):\n",
    "    \"\"\"\n",
    "        Abstract class for nueral network implementations\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Neuron(Module):\n",
    "    \"\"\"\n",
    "        Basic Building Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs) -> None:\n",
    "        \"\"\"\n",
    "            :param n_inputs: Input dimensions\n",
    "        \"\"\"\n",
    "\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(n_inputs)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "        self.parameters = self.w + [self.b]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass\n",
    "            :param x: Input vector of n_inputs dimension\n",
    "        \"\"\"\n",
    "\n",
    "        act = sum((w.data*i for w,i in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "class Layer(Module):\n",
    "    \"\"\"\n",
    "        Layer of neurons\n",
    "        Composite of Neuron Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_out) -> None:\n",
    "        self.neurons = [Neuron(n_in) for _ in range(n_out)]\n",
    "        self.parameters = [param for n in self.neurons for param in n.parameters]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out\n",
    "\n",
    "class MLP(Module):\n",
    "    \"\"\"\n",
    "        Feed Forward Neural Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_outs) -> None:\n",
    "        ins = [n_input] + n_outs\n",
    "        self.layers = [Layer(ins[i], ins[i+1]) for i in range(len(n_outs))]\n",
    "        self.parameters = [param for layer in self.layers for param in layer.parameters]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        temp = x\n",
    "        for layer in self.layers:\n",
    "            temp = layer(temp)\n",
    "        return temp\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "            Initialises the gradients of the parameters before backpropagation\n",
    "        \"\"\"\n",
    "        \n",
    "        for param in self.parameters:\n",
    "            param.grad = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d84076",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "582c179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddbace03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6a41199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Verify Reading Dataset via MnistDataloader class\n",
    "#\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = './mnist'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ea663a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for i in range(len(x_train)):\n",
    "    train.append(np.asarray(x_train[i]))    \n",
    "\n",
    "test = []\n",
    "for i in range(len(x_test)):\n",
    "    test.append(np.asarray(x_test[i]))\n",
    "\n",
    "x_train = train\n",
    "x_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709cc023",
   "metadata": {},
   "source": [
    "28 * 28 -> 14 * 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c72369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(data):\n",
    "    downsample_factor = 4\n",
    "    reshaped_arr = data.reshape(-1, downsample_factor)\n",
    "    downsampled_x = reshaped_arr.mean(axis=1)\n",
    "    reshaped_x = downsampled_x.reshape(14,14)\n",
    "    return reshaped_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc3956a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [downsample(x) for x in x_train]\n",
    "test = [downsample(x) for x in x_test]\n",
    "x_train, x_test  = train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b763116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# # Show some random training and test images \n",
    "# #\n",
    "# images_2_show = []\n",
    "# titles_2_show = []\n",
    "# for i in range(0, 10):\n",
    "#     r = random.randint(1, 60000)\n",
    "#     images_2_show.append(x_train[r])\n",
    "#     titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))    \n",
    "\n",
    "# for i in range(0, 5):\n",
    "#     r = random.randint(1, 10000)\n",
    "#     images_2_show.append(x_test[r])        \n",
    "#     titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))    \n",
    "\n",
    "# show_images(images_2_show, titles_2_show)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2070b95d",
   "metadata": {},
   "source": [
    "1. ```x_train``` is a list (len 60000) of (28, 28) matrices.\n",
    "2. ```y_train``` is a list (len 60000) of int values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ee01cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69461d76",
   "metadata": {},
   "source": [
    "50000 : 10000 : 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85e8b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8257c3",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da151710",
   "metadata": {},
   "source": [
    "14 * 14 -> 7 * 7 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b270df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 14 * 14\n",
    "n_outs = [7 * 7, 1]\n",
    "lr = 0.01\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8079799",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = MSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8f6b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(n_inputs, n_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cffc360e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9703"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlp.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91ff3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(X, y, nn, loss_fn, lr, n_epochs):\n",
    "    for i in range(n_epochs):\n",
    "        pred = [mlp(x)[0] for x in X]\n",
    "        loss = sum(loss_fn(t, p) for t, p in zip(y, pred))\n",
    "\n",
    "        for param in mlp.parameters:\n",
    "            param.grad = 0  # so that the grad of loss calculated is only for this new iteration\n",
    "        loss.backward()\n",
    "\n",
    "        print(f\"{i} : {loss.data}\")\n",
    "\n",
    "        for param in mlp.parameters:\n",
    "            param.data += -lr * param.grad\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d93740",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_1 = training_loop(x_train, y_train, mlp, loss_fn, lr, n_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
